{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f4ce90f",
   "metadata": {},
   "source": [
    "# Transform raw extract to dense representation of non-static medical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236368df-76b1-4962-bcc4-c7ee93087137",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code is modification of:\n",
    "https://github.com/StatNLP/mlhc_2024_prediction_of_causes/blob/main/eicu_experiments/eicu_03_pickle2dense.py\n",
    "The necessary inputs for this script are created via:\n",
    "https://github.com/StatNLP/mlhc_2024_prediction_of_causes/blob/main/mimic_experiments/mimic3_01_preprocess_icu.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fdc7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_list(l, start=0):  # Create: var:ind mappings\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "\n",
    "#f create masked outputs\n",
    "def f(x):\n",
    "    mask   = [0 for i in range(V)]\n",
    "    values = [0 for i in range(V)]\n",
    "    for vv in x:  # tuple of ['vind','value']\n",
    "        v = int(vv[0])-1  # shift index of vind\n",
    "        mask[v] = 1\n",
    "        values[v] = vv[1]  # get value\n",
    "    return values+mask  # concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d4408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mimic-short\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556462e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '<location of pickle file produced by mimic3_02_preprocess_pickle.py>'\n",
    "\n",
    "data, _ , train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\n",
    "del _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2484d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate mean, sd for studentization\n",
    "means_stds = data.groupby(\"variable\").agg({\"mean\":\"first\", \"std\":\"first\"})\n",
    "\n",
    "mean_std_dict = dict()\n",
    "for pos, row in means_stds.iterrows():\n",
    "    mean_std_dict[pos] = (float(row[\"mean\"]), float(row[\"std\"]))\n",
    "\n",
    "with open('../gold_data/'+ dataset_name +'/mean_std_dict.pickle', \"wb\") as outfile:\n",
    "    pickle.dump(mean_std_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e3b5d-e46b-4761-807b-2fa023df38b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce data so that it contains only ts occuring in train, valid or test\n",
    "data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "\n",
    "#Select first 48h hours of each time series.\n",
    "data = data.loc[(data.hour >= 0) & (data.hour <= 48)]\n",
    "\n",
    "# Fix age.\n",
    "data.loc[(data.variable == 'Age') & (data.value > 200), 'value'] = 91.4\n",
    "\n",
    "#fix index data type\n",
    "data['ts_ind'] = data['ts_ind'].astype(int)\n",
    "\n",
    "#In order to calculate N (number of time series) correctly this way. ts_ind must start with 0 and gap less \n",
    "#ALTERNATIVE: N=data['ts_int'].nunique() \n",
    "N = int(data.ts_ind.max() + 1)\n",
    "\n",
    "# remove static variables from data\n",
    "data = data.loc[~data.variable.isin(['Age', 'Gender'])]\n",
    "\n",
    "#number of non-static variables extracted from mimic-db by the previous script. It' important for the mapping and the emedding model\n",
    "V = data['variable'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d931a42e-6133-4e2f-848d-4c6cdde6b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a dense array version of non-static data\n",
    "varis = sorted(list(set(data.variable)))\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "\n",
    "#add numeric indentifier for variable names to data\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "#reduce columns of data and introduce sorting: zeitreihe|variable|zeit\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "#add an oberservation identifier to each time series starting with 0 and incrementing by 1\n",
    "data = data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "data = data.reset_index().rename(columns={'index': 'obs_ind'})\n",
    "data = data.merge(data.groupby('ts_ind').agg({'obs_ind': 'min'}).reset_index().rename(columns={'obs_ind': 'first_obs_ind'}), on='ts_ind')\n",
    "data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "\n",
    "\n",
    "#After this step we have a densification of the first 48h of for each time series\n",
    "for pred_window in tqdm(range(0, 48, 1)):\n",
    "    #selection of 1h time window. Remark: to non-sharp unequal signs\n",
    "    pred_data = data.loc[(data.hour >= (w + pred_window)) & (data.hour <= (w + pred_window + 1))] \n",
    "    #pick first observed measurment within that window\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value': 'first'}).reset_index() \n",
    "    #MH: cast var_id an value to a list and add \n",
    "    pred_data['vind_value' + str(pred_window)] = pred_data[['vind', 'value']].values.tolist() \n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value' + str(pred_window): list}).reset_index()\n",
    "    # turn to dense representation with mask\n",
    "    pred_data['vind_value' + str(pred_window)] = pred_data['vind_value' + str(pred_window)].apply(f)\n",
    "    #add variables 'vind_value<0-47>' storing the dense vectors\n",
    "    if pred_window == 0:\n",
    "        obs_data = pred_data\n",
    "    else:\n",
    "        obs_data = obs_data.merge(pred_data, on = 'ts_ind') \n",
    "\n",
    "\n",
    "#change obs_data so that for each ts obs_ind starts with 0\n",
    "obs_data = obs_data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "obs_data = obs_data.reset_index().rename(columns={'index': 'obs_ind'})\n",
    "obs_data = obs_data.merge(data.groupby('ts_ind').agg({'obs_ind': 'min'}).reset_index().rename(columns={'obs_ind': 'first_obs_ind'}), on='ts_ind')\n",
    "obs_data['obs_ind'] = obs_data['obs_ind'] - obs_data['first_obs_ind']\n",
    "\n",
    "\n",
    "#(1) list(obs_data['vind_valueX']) is a list with #ts length of list with length 262\n",
    "#(2) [... for pred_windows in range(0,48)] creates a list of those lists \n",
    "#so blub is a np array (48, #ts, 262)\n",
    "blub = (np.array(list([list(obs_data['vind_value' + str(pred_window)]) for pred_window in range(0, 48, 1)])))\n",
    "#shape of op (#ts, 48, 262)\n",
    "op = np.swapaxes(blub, 0, 1)\n",
    "\n",
    "#select of train, valid, test indicies\n",
    "train_ind = [x for x in train_ind if x < op.shape[0]]\n",
    "valid_ind = [x for x in valid_ind if x < op.shape[0]]\n",
    "test_ind = [x for x in test_ind if x < op.shape[0]]\n",
    "\n",
    "#split into train, vaild, test based on ts_ind\n",
    "train_input = op[train_ind]\n",
    "valid_input = op[valid_ind]\n",
    "test_input = op[test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "112855ca-4774-44ea-9e87-19aee589fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this block contains instruction for to reduced data sets\n",
    "factor_indices1 = [var_to_ind[x]- 1 for x in [\"HR\", \"RR\", \"SBP\", \"DBP\", \"MBP\", \"O2 Saturation\"]]\n",
    "factor_indices2 = factor_indices1 + [V + x for x in factor_indices1] \n",
    "\n",
    "train_input = op[train_ind][:, :, factor_indices2]\n",
    "valid_input = op[valid_ind][:, :, factor_indices2]\n",
    "test_input = op[test_ind][:, :, factor_indices2] \n",
    "\n",
    "#change V for correct emedding generation\n",
    "V = train_input.shape[2]//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e6dcd-cc63-406a-a3ba-8b1ae29aeb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../gold_data/'+ dataset_name +'/gold_train.pickle', \"wb\") as outfile:\n",
    "    pickle.dump(train_input, outfile, protocol = 4)\n",
    "    \n",
    "with open('../gold_data/'+ dataset_name +'/gold_valid.pickle', \"wb\") as outfile:\n",
    "    pickle.dump(valid_input, outfile, protocol = 4)\n",
    "    \n",
    "with open('../gold_data/'+ dataset_name +'/gold_test.pickle', \"wb\") as outfile:\n",
    "    pickle.dump(test_input, outfile, protocol = 4)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a553eb",
   "metadata": {},
   "source": [
    "# Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b2044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import importlib\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "sys.path.append('LTSFLinear/models')\n",
    "sys.path.append('LTSFLinear/')\n",
    "\n",
    "import InformerAutoregressiveExtract as autoformer\n",
    "\n",
    "embd_model = \"/LTSFLinear/informer_mimic_reduced_3hours.pytorch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cde738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Set the random seed for modules torch, numpy and random.\n",
    "\n",
    "    :param seed: random seed\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e3ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "parser.add_argument('--train_only', type=bool, required=False, default=False, help='perform training on full input dataset without validation and testing')\n",
    "parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=False, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=False, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=3, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=3, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=3, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "\n",
    "#added for reduced data sets\n",
    "parser.add_argument('--enc_in', type=int, default=V*2, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=V, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=V, help='output size') \n",
    "\n",
    "parser.add_argument('--d_model', type=int, default=50, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=False)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "args = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12355c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate emedding model:\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "\n",
    "# Load embedding model weights:\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "model.load_state_dict(torch.load(embd_model))\n",
    "\n",
    "#put embedding model in eval mode:\n",
    "model.eval()\n",
    "\n",
    "\n",
    "data2embd = train_input #<-- train|valid|test_input\n",
    "#funnel data through embedding model:\n",
    "global_list1, global_list2 = [], []\n",
    "for start in tqdm(range(0, len(data2embd), batch_size)):\n",
    "    matrix = torch.tensor(train_input[start:start+batch_size], dtype=torch.float32).cuda() #MH: filter ts and cast to torch.tensor\n",
    "    x = torch.tensor(matrix, dtype=torch.float32).cuda()\n",
    "    liste = []\n",
    "    liste2 = []\n",
    "    for i in range(48//3):\n",
    "        input_matrix = matrix[:, i*3:(i+1)*3] \n",
    "        input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        input_mask = input_matrix[:, :24, V:]\n",
    "        output_matrix = matrix[:, 24:, :V] \n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        output_mask = matrix[:, 24:, V:]\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()   \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "            output_real = output.mean(dim=1).unsqueeze(dim=1)\n",
    "            liste.append(output_real)\n",
    "            liste2.append(input_matrix.unsqueeze(1))\n",
    "          \n",
    "    outputs = torch.concat(liste, dim=1)\n",
    "    inputs = torch.concat(liste2, dim=1)\n",
    "    \n",
    "    for mini in inputs:\n",
    "        global_list1.append(mini)\n",
    "        \n",
    "    for mini in outputs:\n",
    "        global_list2.append(mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712910cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "totallist = []\n",
    "\n",
    "for inp, out in zip(global_list1, global_list2):\n",
    "    totallist.append((inp.to('cpu').numpy(), out.to('cpu').numpy()))\n",
    " \n",
    "#change outfile ending to '-<train|valid|test>.pickle' accordingly to data2embd\n",
    "with open('../embeddings/embd3h_' + dataset_name + '-train.pickle', \"wb\") as outfile:\n",
    "    pickle.dump(totallist, outfile, protocol = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
