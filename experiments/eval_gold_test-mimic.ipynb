{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-termination",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T11:52:56.289283Z",
     "iopub.status.busy": "2023-11-23T11:52:56.288453Z",
     "iopub.status.idle": "2023-11-23T11:52:59.823096Z",
     "shell.execute_reply": "2023-11-23T11:52:59.822088Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c94544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T11:52:59.826920Z",
     "iopub.status.busy": "2023-11-23T11:52:59.826550Z",
     "iopub.status.idle": "2023-11-23T11:52:59.832492Z",
     "shell.execute_reply": "2023-11-23T11:52:59.831649Z"
    }
   },
   "outputs": [],
   "source": [
    "# data settings\n",
    "test_cond = 0\n",
    "sepsis_check = 0\n",
    "\n",
    "if test_cond == 1:\n",
    "    data_path = '/home/mitarb/fracarolli/files/231113_STraTS_new/mimic_iii_preprocessed_new_z_err20092.pkl'\n",
    "    sample_divisor = 100\n",
    "    number_of_epochs = 20\n",
    "else:\n",
    "    data_path = '/home/mitarb/fracarolli/files/231113_STraTS_new/mimic_iii_preprocessed_new_z_err20092.pkl'\n",
    "    sample_divisor = 1\n",
    "    number_of_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5237f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Set the random seed for modules torch, numpy and random.\n",
    "\n",
    "    :param seed: random seed\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-record",
   "metadata": {},
   "source": [
    "## Load forecast dataset into matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fdc7de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T11:52:59.835708Z",
     "iopub.status.busy": "2023-11-23T11:52:59.835482Z",
     "iopub.status.idle": "2023-11-23T11:52:59.842763Z",
     "shell.execute_reply": "2023-11-23T11:52:59.841782Z"
    }
   },
   "outputs": [],
   "source": [
    "def inv_list(l, start=0):  # Create vind\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    mask   = [0 for i in range(V)]\n",
    "    values = [0 for i in range(V)]\n",
    "    for vv in x:  # tuple of ['vind','value']\n",
    "        v = int(vv[0])-1  # shift index of vind\n",
    "        mask[v] = 1\n",
    "        values[v] = vv[1]  # get value\n",
    "    return values+mask  # concat\n",
    "\n",
    "\n",
    "def pad(x):\n",
    "    if len(x) > 880:\n",
    "        print(len(x))\n",
    "    return x+[0]*(fore_max_len-len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533146ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T12:50:22.928639Z",
     "iopub.status.busy": "2023-11-23T12:50:22.928294Z",
     "iopub.status.idle": "2023-11-23T12:54:40.326822Z",
     "shell.execute_reply": "2023-11-23T12:54:40.325636Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "V=131\n",
    "fore_max_len = 880\n",
    "# Read data.\n",
    "# data_path = '/home/mitarb/fracarolli/files/230613_STraTS_preprocessed/mimic_iii_preprocessed.pkl'\n",
    "data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\n",
    "\n",
    "import itertools\n",
    "if sepsis_check:\n",
    "    test_ind = pd.read_csv('infec_test.csv', header=None)\n",
    "    test_ind = list(itertools.chain(*test_ind.values.tolist()))\n",
    "    test_ind = list(map(int, test_ind))\n",
    "    print('hi')\n",
    "\n",
    "means_stds = data.groupby(\"variable\").agg({\"mean\":\"first\", \"std\":\"first\"})\n",
    "#print(means_stds)\n",
    "mean_std_dict = dict()\n",
    "print(means_stds.keys)\n",
    "for pos, row in means_stds.iterrows():\n",
    "    print(pos)\n",
    "    print(row)\n",
    "    mean_std_dict[pos] = (float(row[\"mean\"]), float(row[\"std\"]))\n",
    "print(mean_std_dict)\n",
    "#raise Exception\n",
    "# Filter labeled data in first 24h.\n",
    "data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "data = data.loc[(data.hour >= 0) & (data.hour <= 48)]\n",
    "old_oc = oc\n",
    "oc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# Fix age.\n",
    "data.loc[(data.variable == 'Age') & (data.value > 200), 'value'] = 91.4\n",
    "# Get y and N.\n",
    "y = np.array(oc.sort_values(by='ts_ind')['in_hospital_mortality']).astype('float32')\n",
    "print(y.shape)\n",
    "#raise Exception\n",
    "data['ts_ind'] = data['ts_ind'].astype(int) \n",
    "N = int(data.ts_ind.max() + 1)\n",
    "\n",
    "# Create demographic/static data\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]  # reduce data to non-demo/non-static\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "demo = np.zeros((int(N), D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[int(row.ts_ind), static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds == 0)*1 + (stds != 0)*stds\n",
    "demo = (demo-means)/stds\n",
    "\n",
    "# Get N, V, var_to_ind.\n",
    "varis = sorted(list(set(data.variable)))\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Add obs index.\n",
    "data = data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "data = data.reset_index().rename(columns={'index': 'obs_ind'})\n",
    "data = data.merge(data.groupby('ts_ind').agg({'obs_ind': 'min'}).reset_index().rename(columns={\n",
    "                                                            'obs_ind': 'first_obs_ind'}), on='ts_ind')\n",
    "data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "# Find max_len.\n",
    "data2 = data.loc[(data.hour >= 0) & (data.hour <= 24)]\n",
    "max_len = data2.obs_ind.max()+1\n",
    "print('max_len', max_len)\n",
    "# Generate times_ip and values_ip matrices.\n",
    "times_inp = np.zeros((N, max_len), dtype='float32')\n",
    "values_inp = np.zeros((N, max_len), dtype='float32')\n",
    "varis_inp = np.zeros((N, max_len), dtype='int32')\n",
    "for row in tqdm(data2.itertuples()):\n",
    "    ts_ind = row.ts_ind\n",
    "    l = row.obs_ind\n",
    "    times_inp[ts_ind, l] = row.hour\n",
    "    values_inp[ts_ind, l] = row.value\n",
    "    varis_inp[ts_ind, l] = row.vind\n",
    "\n",
    "w=0\n",
    "\n",
    "fore_in = []\n",
    "\n",
    "pred_data = data.loc[(data.hour>=24)&(data.hour<=48)]\n",
    "pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value':'first'}).reset_index()\n",
    "pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "pred_data = pred_data.groupby('ts_ind').agg({'vind_value':list}).reset_index()\n",
    "pred_data['vind_value'] = pred_data['vind_value'].apply(f)   \n",
    "\n",
    "obs_data = data.loc[(data.hour < 24) & (data.hour >= 0)]\n",
    "resultdict = dict()\n",
    "for ts_ind in obs_data.ts_ind:\n",
    "    resultdict[ts_ind] = [[]]\n",
    "obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "obs_data = obs_data.groupby('ts_ind').agg({'vind': list, 'hour': list, 'value': list}).reset_index()\n",
    "\n",
    "for pred_window  in range(0, 48, 1):\n",
    "    print(pred_window)\n",
    "    pred_data = data.loc[(data.hour >= w+pred_window) & (data.hour <= w+1 +pred_window)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value': 'first'}).reset_index()\n",
    "    pred_data['vind_value'+str(pred_window)] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value'+str(pred_window): list}).reset_index()\n",
    "    pred_data['vind_value'+str(pred_window)] = pred_data['vind_value'+str(pred_window)].apply(f)  # 721 entries with 2*129 vind_values\n",
    "    obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "\n",
    "\n",
    "obs_data = obs_data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "obs_data = obs_data.reset_index().rename(columns={'index': 'obs_ind'})\n",
    "obs_data = obs_data.merge(data.groupby('ts_ind').agg({'obs_ind': 'min'}).reset_index().rename(columns={\n",
    "                                                            'obs_ind': 'first_obs_ind'}), on='ts_ind')\n",
    "obs_data['obs_ind'] = obs_data['obs_ind'] - obs_data['first_obs_ind']\n",
    "\n",
    "\n",
    "N = int(obs_data.ts_ind.max() + 1)\n",
    "times_inp = np.zeros((N, max_len), dtype='float32')\n",
    "values_inp = np.zeros((N, max_len), dtype='float32')\n",
    "varis_inp = np.zeros((N, max_len), dtype='int32')\n",
    "for col in ['vind', 'hour', 'value']:\n",
    "        obs_data[col] = obs_data[col].apply(pad)\n",
    "\n",
    "\n",
    "times_inp=(np.array(list(obs_data.hour)))\n",
    "values_inp=(np.array(list(obs_data.value)))\n",
    "varis_inp=(np.array(list(obs_data.vind)))\n",
    "\n",
    "print(\"times_shape_blub\", times_inp.shape)\n",
    "index=np.array([int(x) for x in list(obs_data.ts_ind)])\n",
    "demo = demo[index]\n",
    "blub = (np.array(list([list(obs_data['vind_value'+str(pred_window)]) for pred_window in range(0, 48, 1)])))\n",
    "print(\"bleb\", blub.shape)\n",
    "op = np.swapaxes(blub, 0, 1)\n",
    "\n",
    "weird_oc = oc.loc[oc.ts_ind.isin(obs_data.ts_ind)]\n",
    "y = np.array(weird_oc.sort_values(by='ts_ind')['in_hospital_mortality']).astype('float32')\n",
    "print(y.shape)\n",
    "data.drop(columns=['obs_ind', 'first_obs_ind'], inplace=True)\n",
    "\n",
    "train_ind = [x for x in train_ind if x < op.shape[0]]\n",
    "valid_ind = [x for x in valid_ind if x < op.shape[0]]\n",
    "test_ind = [x for x in test_ind if x < op.shape[0]]\n",
    "\n",
    "train_input = op[train_ind]\n",
    "valid_input = op[valid_ind]\n",
    "test_input = op[test_ind]\n",
    "\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ip = [ip[train_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "valid_ip = [ip[valid_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "test_ip = [ip[test_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "del demo, times_inp, values_inp, varis_inp  # warum wird demo nicht gelöscht?\n",
    "\n",
    "if test_cond == 1:\n",
    "    tr_ind = [divmod(tr, 12)[0] for tr in train_ind]\n",
    "    va_ind = [divmod(tr, 12)[0] for tr in valid_ind]\n",
    "    te_ind = [divmod(tr, 12)[0] for tr in test_ind]\n",
    "    \n",
    "    train_op = y[tr_ind]  # is a problem for the test case...\n",
    "    valid_op = y[va_ind]\n",
    "    test_op = y[te_ind]\n",
    "else:\n",
    "    train_op = y[train_ind]  # is a problem for the test case...\n",
    "    valid_op = y[valid_ind]\n",
    "    test_op = y[test_ind]\n",
    "print('y is:', y)\n",
    "del y\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f4961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sofa(matrix, var_to_ind): #24x131 matrix\n",
    "    # GCS: min_eye, min_motor, min_verbal = 5, 5, 5\n",
    "    #print(matrix.size())\n",
    "    #raise Exception\n",
    "    key =\"GCS_eye\"\n",
    "    var_to_ind = {x:i-1 for x,i in var_to_ind.items()}\n",
    "    a=matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]\n",
    "    #print(a)\n",
    "    min_eye = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=4)\n",
    "    key = \"GCS_motor\"\n",
    "    min_motor = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=6)\n",
    "    key = \"GCS_verbal\"\n",
    "    min_verbal = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=5)\n",
    "    \n",
    "\n",
    "    GCS = min_eye + min_motor + min_verbal\n",
    "    if GCS > 14: GCS_sofa = 0\n",
    "    elif GCS > 12: GCS_sofa = 1\n",
    "    elif GCS > 9:  GCS_sofa = 2\n",
    "    elif GCS > 5:  GCS_sofa = 3\n",
    "    else: GCS_sofa = 4\n",
    "    #print('GCS_sofa is', GCS_sofa, ';     GCS is', GCS,'; GCS eye', min_eye, '; GCS motor', min_motor, '; GCS verbal', min_verbal)\n",
    "\n",
    "    key = \"Bilirubin (Total)\"\n",
    "    bilir = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    if bilir > 12: bilir_sofa = 4\n",
    "    elif bilir > 6: bilir_sofa = 3\n",
    "    elif bilir > 2: bilir_sofa = 2\n",
    "    elif bilir > 1.2: bilir_sofa = 1\n",
    "    else: bilir_sofa = 0\n",
    "    #print('bilir_Sofa is', bilir_sofa, ';   bilirubin is', bilir)\n",
    "    \n",
    "    # Coagulation (Platelets)\n",
    "    key = \"Platelet Count\"\n",
    "    plate = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=160)\n",
    "    if plate > 150: plate_sofa = 0\n",
    "    elif plate > 100: plate_sofa = 1\n",
    "    elif plate > 50: plate_sofa = 2\n",
    "    elif plate > 20: plate_sofa = 3\n",
    "    else: plate_sofa = 4\n",
    "    #print('plate_sofa is', plate_sofa, ';   platelet count is', plate)\n",
    "    \n",
    "    # print('Urinmenge 24h', sum(data_var[data_var['variable']=='Urine']['value2']))\n",
    "\n",
    "    key = \"Urine\"\n",
    "    urine = sum(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    key = \"Creatinine Blood\"\n",
    "    creat = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    \n",
    "    if (urine < 200) or (creat > 5): renal_sofa = 4\n",
    "    elif  (urine < 500) or (creat > 3.5): renal_sofa = 3\n",
    "    elif creat > 2.0: renal_sofa = 2\n",
    "    elif creat > 1.2: renal_sofa = 1\n",
    "    else: renal_sofa = 0\n",
    "    #print('renal_sofa:',renal_sofa,';       urine 24:',urine,'; creat:', creat)\n",
    "    \n",
    "    CS_data = get_CS(matrix, var_to_ind)\n",
    "    cs_sofa = CS_SOFA(CS_data)\n",
    "    \n",
    "    #cs_sofa = 0\n",
    "    key=\"FiO2\"\n",
    "    fio2 = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    key=\"PO2\"\n",
    "    po2 = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    PaO2FiO2 = 100*po2/fio2\n",
    "    #print(\"size\", PaO2FiO2.size())\n",
    "    PaO2FiO2 = PaO2FiO2[torch.nonzero(PaO2FiO2, as_tuple=True)]\n",
    "    pao2fio2 = min(PaO2FiO2)\n",
    "    if pao2fio2<100: resp=4\n",
    "    elif pao2fio2<200: resp=3\n",
    "    elif pao2fio2<300:resp=2\n",
    "    elif pao2fio2<400:resp=1\n",
    "    else: resp=0\n",
    "    return GCS_sofa, cs_sofa, resp, plate_sofa, bilir_sofa, renal_sofa\n",
    "\n",
    "def get_CS(matrix, var_to_ind):\n",
    "    #data_var = data_pat[data_pat['variable'].isin(['Dobutamine','Dopamine','Epinephrine','Norepinephrine','Weight'])]\n",
    "    #data_var['value2'] = data_var['value']*data_var['std']+data_var['mean']\n",
    "    \n",
    "    #weight = min(data_var[data_var['variable']=='Weight']['value2'], default=80)  # set default weight to 80kg.\n",
    "    key = \"Weight\"\n",
    "    weight = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)], default=80)#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    key = \"Dopamine\"\n",
    "    try:\n",
    "        data_dop = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_dop = data_dop /60/weight*1000\n",
    "    except:\n",
    "        data_dop = 0\n",
    "\n",
    "    key = \"Dobutamine\"\n",
    "    \n",
    "    try:\n",
    "        data_dobu = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_dobu = data_dobu  /60/weight*1000\n",
    "    except:\n",
    "        data_dobu = 0\n",
    "    key = \"Epinephrine\"\n",
    "    \n",
    "    try:\n",
    "        data_epi = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_epi = data_epi  /60/weight*1000\n",
    "    except:\n",
    "        data_epi = 0\n",
    "    key = \"Norepinephrine\"\n",
    "    \n",
    "    try:\n",
    "        data_nore = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_nore = data_nore /60/weight*1000\n",
    "    except:\n",
    "        data_nore = 0\n",
    "        \n",
    "\n",
    "\n",
    "    key = \"SBP\"\n",
    "    SBP = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "\n",
    "    key = \"DBP\"\n",
    "    DBP = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "\n",
    "    MAP = 2/3 * DBP + 1/3 * SBP\n",
    "    MAP = min(MAP[torch.nonzero(MAP, as_tuple=True)], default=100)\n",
    "                 \n",
    "        \n",
    "    return MAP, data_dop, data_dobu, data_epi, data_nore \n",
    "    \n",
    "def CS_SOFA(data):\n",
    "    map = data[0]\n",
    "    dop, dobu, epi, nore = data[1:5]\n",
    "    # print('CS data: mdden', data)\n",
    "    if (dop > 15) or (epi > 0.1) or (nore > 0.01): CS = 4\n",
    "    elif (dop > 5) or (epi > 0) or (nore > 0): CS = 3\n",
    "    elif (dop > 0) or (dobu > 0): CS = 2\n",
    "    elif map < 70: CS = 1\n",
    "    else: CS = 0\n",
    "    # print('CS Sofa is:', CS)\n",
    "    return CS \n",
    "    \n",
    "factor_keys =[\"GCS_eye\", \"GCS_motor\", \"GCS_verbal\", \"Bilirubin (Total)\", \"Platelet Count\", \"Urine\", \"Creatinine Blood\", \"FiO2\", \"PO2\", \"Dopamine\", \"Dobutamine\",\n",
    "\"Epinephrine\", \"Norepinephrine\", \"SBP\", \"DBP\"]\n",
    "factor_indices = sorted([var_to_ind[x]-1 for x in factor_keys])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f1b03",
   "metadata": {},
   "source": [
    "# 8 Sofa Analysis IMS Informer TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26575f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Michi: die ganzen Argparses könnte man noch anders implementieren, wollte es nur schnell zum laufen bringen.\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "parser.add_argument('--train_only', type=bool, required=False, default=False, help='perform training on full input dataset without validation and testing')\n",
    "parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=False, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=False, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=24, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=24, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "# Formers \n",
    "#Michi: bisher nur default=3 zum laufen gebracht\n",
    "parser.add_argument('--embed_type', type=int, default=3, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=262, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=131, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=131, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b7295",
   "metadata": {},
   "source": [
    "# 7 Sofa Analysis DMS Informer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a553eb",
   "metadata": {},
   "source": [
    "# 10 Sofa Analysis IMS Informer Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12355c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.InformerAutoregressiveFull as autoformer\n",
    "import os\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "for model_path in os.listdir(\"results-03June_pretrained/\"):\n",
    "    #if not (model_path.endswith(\"1.model\") or model_path.endswith(\"2.model\")): continue\n",
    "    #if not (\"k80\" in model_path or \"k160\" in model_path) or not(model_path.endswith(\"1.model\")): continue\n",
    "    if not (model_path.endswith(\".model\")): continue\n",
    "    print(model_path)\n",
    "    model.load_state_dict(torch.load(\"results-03June_pretrained/\"+model_path))\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    #print(test_input)\n",
    "    pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "    loss_list = []\n",
    "    test_loss = 0\n",
    "    test_loss_part1 = 0\n",
    "    test_loss_part2 = 0\n",
    "    sofa_losses = []\n",
    "    test_loss_sofa_variables = 0\n",
    "    accuracy_list = []\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    test_loss_sum_square_total = 0\n",
    "    test_loss_part1_sum_square_total = 0\n",
    "    test_loss_part2_sum_square_total = 0\n",
    "    test_loss_sofa_sum_square_total = 0\n",
    "    for start in pbar:\n",
    "        #print(start)\n",
    "        matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "        input_matrix = matrix[:, :24]\n",
    "\n",
    "        #torch.Size([32, 24, 129])\n",
    "        input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        input_mask = input_matrix[:, :24, 131:]\n",
    "        output_matrix = matrix[:, 24:, :131]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        output_mask = matrix[:, 24:, 131:]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "        with torch.no_grad():\n",
    "            output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "            loss = output_mask[:, -args.pred_len:, :]*(\n",
    "            output-output_matrix[:, -args.pred_len:, :])**2\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "            a = (get_sofa(outpu*mask, var_to_ind))\n",
    "            b = (get_sofa(real, var_to_ind))\n",
    "            c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "            prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "            gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "            accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "            if gold_sofa_difference:\n",
    "                if prediction_sofa_difference: TP +=1\n",
    "                else: FP += 1\n",
    "            else:\n",
    "                if prediction_sofa_difference: TN +=1\n",
    "                else: FN += 1\n",
    "            c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "            sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "        loss_old = loss\n",
    "        loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "        loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "        loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "        test_loss+=loss\n",
    "        test_loss_part1+=loss_part1\n",
    "        test_loss_part2+=loss_part2\n",
    "        test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "        test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_p = test_loss/len(test_input)\n",
    "    se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "    test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "    test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "    test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "    sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "    accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "    se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "    se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "    se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "    se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "    print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "    print('Informer_IMS_SF_B',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "    model_name = \"results-03June_pretrained/\"+model_path+'&'\n",
    "    MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "    SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "    MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "    MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "    SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "    print(model_name,MSE_string, SOFA_string, sep='',file=open('table_higher_sample_silver_03June1.txt','a'))\n",
    "    print(model_name,MSE8_string, MSE16_string, sep='',file=open('table_higher_sample_silver_03June2.txt','a'))\n",
    "    print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('table_higher_sample_silver_03June3.txt','a')) \n",
    "    print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('table_higher_sample_silver_03June4.txt','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.InformerAutoregressiveFull as autoformer\n",
    "import os\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "for model_path in os.listdir(\"results-03June_pretrained_lower/\"):\n",
    "    #if not (model_path.endswith(\"1.model\") or model_path.endswith(\"2.model\")): continue\n",
    "    #if not (\"k80\" in model_path or \"k160\" in model_path) or not(model_path.endswith(\"1.model\")): continue\n",
    "    if not (model_path.endswith(\".model\")): continue\n",
    "    print(model_path)\n",
    "    model.load_state_dict(torch.load(\"results-03June_pretrained_lower/\"+model_path))\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    #print(test_input)\n",
    "    pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "    loss_list = []\n",
    "    test_loss = 0\n",
    "    test_loss_part1 = 0\n",
    "    test_loss_part2 = 0\n",
    "    sofa_losses = []\n",
    "    test_loss_sofa_variables = 0\n",
    "    accuracy_list = []\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    test_loss_sum_square_total = 0\n",
    "    test_loss_part1_sum_square_total = 0\n",
    "    test_loss_part2_sum_square_total = 0\n",
    "    test_loss_sofa_sum_square_total = 0\n",
    "    for start in pbar:\n",
    "        #print(start)\n",
    "        matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "        input_matrix = matrix[:, :24]\n",
    "\n",
    "        #torch.Size([32, 24, 129])\n",
    "        input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        input_mask = input_matrix[:, :24, 131:]\n",
    "        output_matrix = matrix[:, 24:, :131]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        output_mask = matrix[:, 24:, 131:]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "        with torch.no_grad():\n",
    "            output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "            loss = output_mask[:, -args.pred_len:, :]*(\n",
    "            output-output_matrix[:, -args.pred_len:, :])**2\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "            a = (get_sofa(outpu*mask, var_to_ind))\n",
    "            b = (get_sofa(real, var_to_ind))\n",
    "            c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "            prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "            gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "            accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "            if gold_sofa_difference:\n",
    "                if prediction_sofa_difference: TP +=1\n",
    "                else: FP += 1\n",
    "            else:\n",
    "                if prediction_sofa_difference: TN +=1\n",
    "                else: FN += 1\n",
    "            c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "            sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "        loss_old = loss\n",
    "        loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "        loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "        loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "        test_loss+=loss\n",
    "        test_loss_part1+=loss_part1\n",
    "        test_loss_part2+=loss_part2\n",
    "        test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "        test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_p = test_loss/len(test_input)\n",
    "    se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "    test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "    test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "    test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "    sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "    accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "    se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "    se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "    se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "    se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "    print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "    print('Informer_IMS_SF_B',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "    model_name = \"results-03June_pretrained_lower/\"+model_path+'&'\n",
    "    MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "    SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "    MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "    MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "    SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "    print(model_name,MSE_string, SOFA_string, sep='',file=open('table_higher_sample_silver_03June1.txt','a'))\n",
    "    print(model_name,MSE8_string, MSE16_string, sep='',file=open('table_higher_sample_silver_03June2.txt','a'))\n",
    "    print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('table_higher_sample_silver_03June3.txt','a')) \n",
    "    print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('table_higher_sample_silver_03June4.txt','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb8cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.InformerAutoregressiveFull as autoformer\n",
    "import os\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "for model_path in os.listdir(\"results-03June_mixture/\"):\n",
    "    #if not (model_path.endswith(\"1.model\") or model_path.endswith(\"2.model\")): continue\n",
    "    #if not (\"k80\" in model_path or \"k160\" in model_path) or not(model_path.endswith(\"1.model\")): continue\n",
    "    if not (model_path.endswith(\".model\")): continue\n",
    "    print(model_path)\n",
    "    model.load_state_dict(torch.load(\"results-03June_mixture/\"+model_path))\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    #print(test_input)\n",
    "    pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "    loss_list = []\n",
    "    test_loss = 0\n",
    "    test_loss_part1 = 0\n",
    "    test_loss_part2 = 0\n",
    "    sofa_losses = []\n",
    "    test_loss_sofa_variables = 0\n",
    "    accuracy_list = []\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    test_loss_sum_square_total = 0\n",
    "    test_loss_part1_sum_square_total = 0\n",
    "    test_loss_part2_sum_square_total = 0\n",
    "    test_loss_sofa_sum_square_total = 0\n",
    "    for start in pbar:\n",
    "        #print(start)\n",
    "        matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "        input_matrix = matrix[:, :24]\n",
    "\n",
    "        #torch.Size([32, 24, 129])\n",
    "        input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        input_mask = input_matrix[:, :24, 131:]\n",
    "        output_matrix = matrix[:, 24:, :131]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        output_mask = matrix[:, 24:, 131:]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "        with torch.no_grad():\n",
    "            output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "            loss = output_mask[:, -args.pred_len:, :]*(\n",
    "            output-output_matrix[:, -args.pred_len:, :])**2\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "            a = (get_sofa(outpu*mask, var_to_ind))\n",
    "            b = (get_sofa(real, var_to_ind))\n",
    "            c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "            prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "            gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "            accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "            if gold_sofa_difference:\n",
    "                if prediction_sofa_difference: TP +=1\n",
    "                else: FP += 1\n",
    "            else:\n",
    "                if prediction_sofa_difference: TN +=1\n",
    "                else: FN += 1\n",
    "            c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "            sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "        loss_old = loss\n",
    "        loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "        loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "        loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "        test_loss+=loss\n",
    "        test_loss_part1+=loss_part1\n",
    "        test_loss_part2+=loss_part2\n",
    "        test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "        test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_p = test_loss/len(test_input)\n",
    "    se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "    test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "    test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "    test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "    sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "    accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "    se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "    se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "    se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "    se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "    print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "    print('Informer_IMS_SF_B',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "    model_name = \"results-03June_mixture/\"+model_path+'&'\n",
    "    MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "    SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "    MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "    MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "    SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "    print(model_name,MSE_string, SOFA_string, sep='',file=open('table_higher_sample_silver_03June1.txt','a'))\n",
    "    print(model_name,MSE8_string, MSE16_string, sep='',file=open('table_higher_sample_silver_03June2.txt','a'))\n",
    "    print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('table_higher_sample_silver_03June3.txt','a')) \n",
    "    print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('table_higher_sample_silver_03June4.txt','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.InformerAutoregressiveFull as autoformer\n",
    "import os\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "for model_path in os.listdir(\"results-03June_schedule/\"):\n",
    "    #if not (model_path.endswith(\"1.model\") or model_path.endswith(\"2.model\")): continue\n",
    "    #if not (\"k80\" in model_path or \"k160\" in model_path) or not(model_path.endswith(\"1.model\")): continue\n",
    "    if not (model_path.endswith(\".model\")): continue\n",
    "    print(model_path)\n",
    "    model.load_state_dict(torch.load(\"results-03June_schedule/\"+model_path))\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    #print(test_input)\n",
    "    pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "    loss_list = []\n",
    "    test_loss = 0\n",
    "    test_loss_part1 = 0\n",
    "    test_loss_part2 = 0\n",
    "    sofa_losses = []\n",
    "    test_loss_sofa_variables = 0\n",
    "    accuracy_list = []\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    test_loss_sum_square_total = 0\n",
    "    test_loss_part1_sum_square_total = 0\n",
    "    test_loss_part2_sum_square_total = 0\n",
    "    test_loss_sofa_sum_square_total = 0\n",
    "    for start in pbar:\n",
    "        #print(start)\n",
    "        matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "        input_matrix = matrix[:, :24]\n",
    "\n",
    "        #torch.Size([32, 24, 129])\n",
    "        input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        input_mask = input_matrix[:, :24, 131:]\n",
    "        output_matrix = matrix[:, 24:, :131]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        output_mask = matrix[:, 24:, 131:]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "        with torch.no_grad():\n",
    "            output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "            loss = output_mask[:, -args.pred_len:, :]*(\n",
    "            output-output_matrix[:, -args.pred_len:, :])**2\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "            a = (get_sofa(outpu*mask, var_to_ind))\n",
    "            b = (get_sofa(real, var_to_ind))\n",
    "            c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "            prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "            gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "            accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "            if gold_sofa_difference:\n",
    "                if prediction_sofa_difference: TP +=1\n",
    "                else: FP += 1\n",
    "            else:\n",
    "                if prediction_sofa_difference: TN +=1\n",
    "                else: FN += 1\n",
    "            c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "            sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "        loss_old = loss\n",
    "        loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "        loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "        loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "        test_loss+=loss\n",
    "        test_loss_part1+=loss_part1\n",
    "        test_loss_part2+=loss_part2\n",
    "        test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "        test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_p = test_loss/len(test_input)\n",
    "    se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "    test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "    test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "    test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "    sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "    accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "    se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "    se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "    se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "    se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "    print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "    print('Informer_IMS_SF_B',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "    model_name = \"results-03June_schedule/\"+model_path+'&'\n",
    "    MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "    SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "    MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "    MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "    SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "    print(model_name,MSE_string, SOFA_string, sep='',file=open('table_higher_sample_silver_03June1.txt','a'))\n",
    "    print(model_name,MSE8_string, MSE16_string, sep='',file=open('table_higher_sample_silver_03June2.txt','a'))\n",
    "    print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('table_higher_sample_silver_03June3.txt','a')) \n",
    "    print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('table_higher_sample_silver_03June4.txt','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f199edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.InformerAutoregressiveFull as autoformer\n",
    "import os\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "for model_path in os.listdir(\"mimic_full/\"):\n",
    "    #if not (model_path.endswith(\"1.model\") or model_path.endswith(\"2.model\")): continue\n",
    "    #if not (\"k80\" in model_path or \"k160\" in model_path) or not(model_path.endswith(\"1.model\")): continue\n",
    "    if not (model_path.endswith(\".model\")): continue\n",
    "    print(model_path)\n",
    "    model.load_state_dict(torch.load(\"mimic_full/\"+model_path))\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    #print(test_input)\n",
    "    pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "    loss_list = []\n",
    "    test_loss = 0\n",
    "    test_loss_part1 = 0\n",
    "    test_loss_part2 = 0\n",
    "    sofa_losses = []\n",
    "    test_loss_sofa_variables = 0\n",
    "    accuracy_list = []\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    test_loss_sum_square_total = 0\n",
    "    test_loss_part1_sum_square_total = 0\n",
    "    test_loss_part2_sum_square_total = 0\n",
    "    test_loss_sofa_sum_square_total = 0\n",
    "    for start in pbar:\n",
    "        #print(start)\n",
    "        matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "        input_matrix = matrix[:, :24]\n",
    "\n",
    "        #torch.Size([32, 24, 129])\n",
    "        input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        input_mask = input_matrix[:, :24, 131:]\n",
    "        output_matrix = matrix[:, 24:, :131]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        output_mask = matrix[:, 24:, 131:]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "        with torch.no_grad():\n",
    "            output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "            loss = output_mask[:, -args.pred_len:, :]*(\n",
    "            output-output_matrix[:, -args.pred_len:, :])**2\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "            a = (get_sofa(outpu*mask, var_to_ind))\n",
    "            b = (get_sofa(real, var_to_ind))\n",
    "            c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "            prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "            gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "            accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "            if gold_sofa_difference:\n",
    "                if prediction_sofa_difference: TP +=1\n",
    "                else: FP += 1\n",
    "            else:\n",
    "                if prediction_sofa_difference: TN +=1\n",
    "                else: FN += 1\n",
    "            c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "            sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "        loss_old = loss\n",
    "        loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "        loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "        loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "        test_loss+=loss\n",
    "        test_loss_part1+=loss_part1\n",
    "        test_loss_part2+=loss_part2\n",
    "        test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "        test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_p = test_loss/len(test_input)\n",
    "    se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "    test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "    test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "    test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "    sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "    accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "    se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "    se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "    se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "    se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "    print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "    print('Informer_IMS_SF_B',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "    model_name = model_path+'&'\n",
    "    MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "    SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "    MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "    MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "    SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "    print(model_name,MSE_string, SOFA_string, sep='',file=open('mimic_full.txt','a'))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import models.InformerAutoregressiveFull as autoformer\n",
    "import os\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "for model_path in os.listdir(\"mimic_full_finetune/\"):\n",
    "    #if not (model_path.endswith(\"1.model\") or model_path.endswith(\"2.model\")): continue\n",
    "    #if not (\"k80\" in model_path or \"k160\" in model_path) or not(model_path.endswith(\"1.model\")): continue\n",
    "    if not (model_path.endswith(\".model\")): continue\n",
    "    print(model_path)\n",
    "    model.load_state_dict(torch.load(\"mimic_full_finetune/\"+model_path))\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    #print(test_input)\n",
    "    pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "    loss_list = []\n",
    "    test_loss = 0\n",
    "    test_loss_part1 = 0\n",
    "    test_loss_part2 = 0\n",
    "    sofa_losses = []\n",
    "    test_loss_sofa_variables = 0\n",
    "    accuracy_list = []\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    test_loss_sum_square_total = 0\n",
    "    test_loss_part1_sum_square_total = 0\n",
    "    test_loss_part2_sum_square_total = 0\n",
    "    test_loss_sofa_sum_square_total = 0\n",
    "    for start in pbar:\n",
    "        #print(start)\n",
    "        matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "        input_matrix = matrix[:, :24]\n",
    "\n",
    "        #torch.Size([32, 24, 129])\n",
    "        input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        input_mask = input_matrix[:, :24, 131:]\n",
    "        output_matrix = matrix[:, 24:, :131]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        output_mask = matrix[:, 24:, 131:]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "        with torch.no_grad():\n",
    "            output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "            loss = output_mask[:, -args.pred_len:, :]*(\n",
    "            output-output_matrix[:, -args.pred_len:, :])**2\n",
    "        loss_list.extend(loss.sum(axis=-1).mean(axis=-1).detach().cpu().tolist())\n",
    "        for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "            a = (get_sofa(outpu*mask, var_to_ind))\n",
    "            b = (get_sofa(real, var_to_ind))\n",
    "            c = (get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "            prediction_sofa_difference = (sum(a)-sum(c)) >= 2\n",
    "            gold_sofa_difference = (sum(b)-sum(c)) >= 2\n",
    "            accuracy_list.append(prediction_sofa_difference == gold_sofa_difference)\n",
    "            if gold_sofa_difference:\n",
    "                if prediction_sofa_difference: TP +=1\n",
    "                else: FP += 1\n",
    "            else:\n",
    "                if prediction_sofa_difference: TN +=1\n",
    "                else: FN += 1\n",
    "            c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32)\n",
    "            sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "        loss_old = loss\n",
    "        loss = loss.sum(axis=-1).mean(axis=-1).sum()\n",
    "        loss_part1 = loss_old[:, :8].sum(axis=-1).mean()\n",
    "        loss_part2 = loss_old[:, 8:].sum(axis=-1).mean()\n",
    "        test_loss+=loss\n",
    "        test_loss_part1+=loss_part1\n",
    "        test_loss_part2+=loss_part2\n",
    "        test_loss_sofa_variables+=loss_old[:, :, factor_indices].sum(axis=-1).mean()\n",
    "        test_loss_sum_square_total += torch.sum(loss_old.sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part1_sum_square_total += torch.sum(loss_old[:, :8].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_part2_sum_square_total += torch.sum(loss_old[:, 8:].sum(axis=-1).mean(axis=-1)**2)\n",
    "        test_loss_sofa_sum_square_total += torch.sum(loss_old[:, :, factor_indices].sum(axis=-1).mean(axis=-1)**2)\n",
    "    test_loss_p = test_loss/len(test_input)\n",
    "    se_loss_p = torch.sqrt(test_loss_sum_square_total / len(test_input) - test_loss_p**2)/len(test_input)\n",
    "    test_loss_part1_p = test_loss_part1*batch_size/len(test_input)\n",
    "    test_loss_part2_p = test_loss_part2*batch_size/len(test_input)\n",
    "    test_loss_sofa_variables_p = test_loss_sofa_variables*batch_size/len(test_input)\n",
    "    sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "    accuracy_sepsis = sum(accuracy_list)/len(accuracy_list)\n",
    "    se_accuracy_sepsis = np.sqrt(accuracy_sepsis * (1-accuracy_sepsis)/len(test_input))\n",
    "    se_loss_part1_p = torch.sqrt(test_loss_part1_sum_square_total / len(test_input) - test_loss_part1_p**2)/len(test_input)\n",
    "    se_loss_part2_p = torch.sqrt(test_loss_part2_sum_square_total / len(test_input) - test_loss_part2_p**2)/len(test_input)\n",
    "    se_loss_sofa_p  = torch.sqrt(test_loss_sofa_sum_square_total / len(test_input)  - test_loss_sofa_variables_p**2)/len(test_input)\n",
    "    print(test_loss_p, test_loss_part1_p, test_loss_part2_p, sofa_loss, test_loss_sofa_variables_p, accuracy_sepsis)\n",
    "    print('Informer_IMS_SF_B',test_loss_p.item(), test_loss_part1_p.item(), test_loss_part2_p.item(), sofa_loss, test_loss_sofa_variables_p.item(), accuracy_sepsis, sep=',',file=open('results.txt','a'))\n",
    "\n",
    "    model_name = model_path+'&'\n",
    "    MSE_string = \"{:.3f}\".format(test_loss_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_p.item()-1.96*se_loss_p.item()) +','+ \"{:.3f}\".format(test_loss_p.item()+1.96*se_loss_p.item()) + ']}$&'\n",
    "    SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*se_loss_sofa_p.item())+','+\"{:.3f}\".format(sofa_loss+1.96*se_loss_sofa_p.item())+ ']}$&'\n",
    "    MSE8_string = \"{:.3f}\".format(test_loss_part1_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part1_p.item()-1.96*se_loss_part1_p.item()) +','+ \"{:.3f}\".format(test_loss_part1_p.item()+1.96*se_loss_part1_p.item()) + ']}$&'\n",
    "    MSE16_string = \"{:.3f}\".format(test_loss_part2_p.item()) + \"$_{[\" + \"{:.3f}\".format(test_loss_part2_p.item()-1.96*se_loss_part2_p.item()) +','+ \"{:.3f}\".format(test_loss_part2_p.item()+1.96*se_loss_part2_p.item()) + ']}$&'\n",
    "    SOFA_acc_string = \"{:.2f}\".format(accuracy_sepsis*100)+ '$_{['+\"{:.2f}\".format(100*(accuracy_sepsis-1.96*se_accuracy_sepsis))+','+\"{:.2f}\".format(100*(accuracy_sepsis+1.96*se_accuracy_sepsis))+ ']}$&'\n",
    "    print(model_name,MSE_string, SOFA_string, sep='',file=open('mimic_full_finetune.txt','a'))\n",
    "    #print(model_name,MSE8_string, MSE16_string, sep='',file=open('table_higher_sample_silver_03June2.txt','a'))\n",
    "    #print(model_name,TP,'&',TN,'&',FP,'&',FN,'&', TP+TN+FP+FN,'&', len(test_input), sep='',file=open('table_higher_sample_silver_03June3.txt','a')) \n",
    "    #print(model_name,len(test_input),'&',SOFA_acc_string, sep='',file=open('table_higher_sample_silver_03June4.txt','a'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
