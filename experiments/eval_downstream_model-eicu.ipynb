{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d86537",
   "metadata": {},
   "source": [
    "*ToDo*\n",
    "\n",
    "02) pred_window und obs_window ausprobieren.\n",
    "03) output nochmal genau anschauen.\n",
    "04) was ist maskiert anschauen - und wie genau?\n",
    "05) VRAM wenig ausgelastet. Batch size mal mit 320 ausprobieren, aber auch durchziehen.\n",
    "06) Task pred: scheduler einfÃ¼gen.\n",
    "07) Analyse each var (sparsity)\n",
    "08) loss per var/quality of varis loss (is this already ablation to forecast only one var)\n",
    "09) Include sepsis definition\n",
    "10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "driven-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5237f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Set the random seed for modules torch, numpy and random.\n",
    "\n",
    "    :param seed: random seed\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-record",
   "metadata": {},
   "source": [
    "## Load forecast dataset into matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c06d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_std_dict = pickle.load(open(\"/home/mitarb/hagmann/projects/llm_circ/imputed_data/eicu-full/mean_std_dict.pickle\", \"rb\"))\n",
    "var_to_ind = pickle.load(open(\"/home/mitarb/hagmann/projects/llm_circ/imputed_data/eicu-full/var2ind.pickle\", \"rb\"))\n",
    "#test_input = joblib.load(open(\"/home/mitarb/hagmann/projects/llm_circ/imputed_data/eicu-full/imputed_test.pickle\", 'rb'))\n",
    "test_input = pickle.load(open(\"/home/mitarb/hagmann/projects/structures_compositionality/gold_data/eicu-full/gold_test.pickle\", 'rb'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa696ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sofa(matrix, var_to_ind): #24xV matrix\n",
    "    # GCS: min_eye, min_motor, min_verbal = 5, 5, 5\n",
    "    #print(matrix.size())\n",
    "    #raise Exception\n",
    "    key =\"GCS eye\"\n",
    "    var_to_ind = {x:i-1 for x,i in var_to_ind.items()}\n",
    "    a=matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]\n",
    "    #print(a)\n",
    "    min_eye = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=4)\n",
    "    key = \"GCS motor\"\n",
    "    min_motor = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=6)\n",
    "    key = \"GCS verbal\"\n",
    "    min_verbal = min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=5)\n",
    "    \n",
    "\n",
    "    GCS = min_eye + min_motor + min_verbal\n",
    "    if GCS > 14: GCS_sofa = 0\n",
    "    elif GCS > 12: GCS_sofa = 1\n",
    "    elif GCS > 9:  GCS_sofa = 2\n",
    "    elif GCS > 5:  GCS_sofa = 3\n",
    "    else: GCS_sofa = 4\n",
    "    #print('GCS_sofa is', GCS_sofa, ';     GCS is', GCS,'; GCS eye', min_eye, '; GCS motor', min_motor, '; GCS verbal', min_verbal)\n",
    "\n",
    "    key = \"Bilirubin (Total)\"\n",
    "    bilir = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    if bilir > 12: bilir_sofa = 4\n",
    "    elif bilir > 6: bilir_sofa = 3\n",
    "    elif bilir > 2: bilir_sofa = 2\n",
    "    elif bilir > 1.2: bilir_sofa = 1\n",
    "    else: bilir_sofa = 0\n",
    "    #print('bilir_Sofa is', bilir_sofa, ';   bilirubin is', bilir)\n",
    "    \n",
    "    # Coagulation (Platelets)\n",
    "    key = \"Platelets\"\n",
    "    plate = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=160)\n",
    "    if plate > 150: plate_sofa = 0\n",
    "    elif plate > 100: plate_sofa = 1\n",
    "    elif plate > 50: plate_sofa = 2\n",
    "    elif plate > 20: plate_sofa = 3\n",
    "    else: plate_sofa = 4\n",
    "    #print('plate_sofa is', plate_sofa, ';   platelet count is', plate)\n",
    "    \n",
    "    # print('Urinmenge 24h', sum(data_var[data_var['variable']=='Urine']['value2']))\n",
    "\n",
    "    key = \"Urine\"\n",
    "    urine = sum(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    key = \"Creatinine (Blood)\"\n",
    "    creat = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    \n",
    "    if (urine < 200) or (creat > 5): renal_sofa = 4\n",
    "    elif  (urine < 500) or (creat > 3.5): renal_sofa = 3\n",
    "    elif creat > 2.0: renal_sofa = 2\n",
    "    elif creat > 1.2: renal_sofa = 1\n",
    "    else: renal_sofa = 0\n",
    "    #print('renal_sofa:',renal_sofa,';       urine 24:',urine,'; creat:', creat)\n",
    "    \n",
    "    CS_data = get_CS(matrix, var_to_ind)\n",
    "    cs_sofa = CS_SOFA(CS_data)\n",
    "    \n",
    "    #cs_sofa = 0\n",
    "    key=\"FiO2\"\n",
    "    fio2 = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    key=\"PaO2\"\n",
    "    po2 = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "    PaO2FiO2 = 100*po2/fio2\n",
    "    #print(\"size\", PaO2FiO2.size())\n",
    "    PaO2FiO2 = PaO2FiO2[torch.nonzero(PaO2FiO2, as_tuple=True)]\n",
    "    pao2fio2 = min(PaO2FiO2)\n",
    "    if pao2fio2<100: resp=4\n",
    "    elif pao2fio2<200: resp=3\n",
    "    elif pao2fio2<300:resp=2\n",
    "    elif pao2fio2<400:resp=1\n",
    "    else: resp=0\n",
    "    return GCS_sofa, cs_sofa, resp, plate_sofa, bilir_sofa, renal_sofa\n",
    "\n",
    "def get_CS(matrix, var_to_ind):\n",
    "    #data_var = data_pat[data_pat['variable'].isin(['Dobutamine','Dopamine','Epinephrine','Norepinephrine','Weight'])]\n",
    "    #data_var['value2'] = data_var['value']*data_var['std']+data_var['mean']\n",
    "    \n",
    "    #weight = min(data_var[data_var['variable']=='Weight']['value2'], default=80)  # set default weight to 80kg.\n",
    "    key = \"Weight\"\n",
    "    weight = 80# min(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)], default=80)#*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "    key = \"d Dopamine ratio\"\n",
    "    try:\n",
    "        data_dop = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_dop = data_dop /60/weight*1000\n",
    "    except:\n",
    "        data_dop = 0\n",
    "\n",
    "    key = \"d Dobutamine ratio\"\n",
    "    \n",
    "    try:\n",
    "        data_dobu = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_dobu = data_dobu  /60/weight*1000\n",
    "    except:\n",
    "        data_dobu = 0\n",
    "    key = \"d Epinephrine ratio\"\n",
    "    \n",
    "    try:\n",
    "        data_epi = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_epi = data_epi  /60/weight*1000\n",
    "    except:\n",
    "        data_epi = 0\n",
    "    key = \"d Norepinephrine ratio\"\n",
    "    \n",
    "    try:\n",
    "        data_nore = max(matrix[:, var_to_ind[key]][torch.nonzero(matrix[:, var_to_ind[key]], as_tuple=True)]*mean_std_dict[key][1]+mean_std_dict[key][0], default=1)\n",
    "        data_nore = data_nore /60/weight*1000\n",
    "    except:\n",
    "        data_nore = 0\n",
    "        \n",
    "\n",
    "\n",
    "    key = \"SBP\"\n",
    "    SBP = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "\n",
    "    key = \"DBP\"\n",
    "    DBP = (matrix[:, var_to_ind[key]]*mean_std_dict[key][1]+mean_std_dict[key][0])\n",
    "\n",
    "    MAP = 2/3 * DBP + 1/3 * SBP\n",
    "    MAP = min(MAP[torch.nonzero(MAP, as_tuple=True)], default=100)\n",
    "                 \n",
    "        \n",
    "    return MAP, data_dop, data_dobu, data_epi, data_nore \n",
    "    \n",
    "def CS_SOFA(data):\n",
    "    map = data[0]\n",
    "    dop, dobu, epi, nore = data[1:5]\n",
    "    # print('CS data: mdden', data)\n",
    "    if (dop > 15) or (epi > 0.1) or (nore > 0.01): CS = 4\n",
    "    elif (dop > 5) or (epi > 0) or (nore > 0): CS = 3\n",
    "    elif (dop > 0) or (dobu > 0): CS = 2\n",
    "    elif map < 70: CS = 1\n",
    "    else: CS = 0\n",
    "    # print('CS Sofa is:', CS)\n",
    "    return CS \n",
    "    \n",
    "factor_keys =[\"GCS eye\", \"GCS motor\", \"GCS verbal\", \"Bilirubin (Total)\", \"Platelets\", \"Urine\", \"Creatinine (Blood)\", \"FiO2\", \"PaO2\", \"d Dopamine ratio\", \"d Dobutamine ratio\",\n",
    "\"d Epinephrine ratio\", \"d Norepinephrine ratio\", \"SBP\", \"DBP\"]\n",
    "factor_indices = sorted([var_to_ind[x]-1 for x in factor_keys])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875c7053",
   "metadata": {},
   "source": [
    "# Crossinteractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f1b03",
   "metadata": {},
   "source": [
    "# 8 Sofa Analysis IMS Informer TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26575f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading of joblib completed\n"
     ]
    }
   ],
   "source": [
    "d_model, e_layers, d_layers, d_ff = 256, 2, 2, 2048\n",
    "number_of_epochs = 100\n",
    "V, D = 98, 17\n",
    "fore_max_len = 2640\n",
    "print('loading of joblib completed')\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "parser.add_argument('--train_only', type=bool, required=False, default=False, help='perform training on full input dataset without validation and testing')\n",
    "parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=False, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=False, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=24, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=0, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=24, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "# Formers \n",
    "#Michi: bisher nur default=3 zum laufen gebracht\n",
    "parser.add_argument('--embed_type', type=int, default=3, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=V*2, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=V, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=V, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=d_model, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=e_layers, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=d_layers, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=d_ff, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b7295",
   "metadata": {},
   "source": [
    "# 7 Sofa Analysis DMS Informer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a553eb",
   "metadata": {},
   "source": [
    "# 10 Sofa Analysis IMS Informer Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8656c4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutmix-s43-SILVER_ONLY_True_n_246080000_2_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:22<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s43_w1_f4-SILVER_ONLY_True_n_246080000_2_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_1_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:22<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_embedding3h-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_2_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold_train-SILVER_ONLY_True_n_246080000_2_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s41_w1_f4-SILVER_ONLY_True_n_246080000_1_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s43_w1_f4-SILVER_ONLY_True_n_246080000_3_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_embedding3h-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_1_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutmix-s41-SILVER_ONLY_True_n_246080000_3_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:22<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutmix-s42-SILVER_ONLY_True_n_246080000_2_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutmix-s41-SILVER_ONLY_True_n_246080000_2_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:22<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_2_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_embedding3h-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_3_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:22<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutmix-s43-SILVER_ONLY_True_n_246080000_3_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s41_w1_f4-SILVER_ONLY_True_n_246080000_2_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s41_w1_f4-SILVER_ONLY_True_n_246080000_3_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutmix-s43-SILVER_ONLY_True_n_246080000_1_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:22<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_3_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold_train-SILVER_ONLY_True_n_246080000_3_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:22<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutmix-s42-SILVER_ONLY_True_n_246080000_3_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s43_w1_f4-SILVER_ONLY_True_n_246080000_1_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold_train-SILVER_ONLY_True_n_246080000_1_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutmix-s41-SILVER_ONLY_True_n_246080000_1_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutmix-s42-SILVER_ONLY_True_n_246080000_1_PRETRAINED_MODEL_NONE.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import models.InformerAutoregressiveFullRegression as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "import math\n",
    "for model_path in os.listdir(\"eicu_regression_results_high_lr/\"):\n",
    "    #if not (model_path.endswith(\"1.model\") or model_path.endswith(\"2.model\")): continue\n",
    "    #if not (\"k80\" in model_path or \"k160\" in model_path) or not(model_path.endswith(\"1.model\")): continue\n",
    "    if not (model_path.endswith(\".model\")): continue\n",
    "    print(model_path)\n",
    "    model.load_state_dict(torch.load(\"eicu_regression_results_high_lr/\"+model_path))\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    #print(test_input)\n",
    "    pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "    loss_list = []\n",
    "    test_loss = 0\n",
    "    test_loss_part1 = 0\n",
    "    test_loss_part2 = 0\n",
    "    sofa_losses = []\n",
    "    test_loss_sofa_variables = 0\n",
    "    accuracy_list = []\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    test_loss_sum_square_total = 0\n",
    "    test_loss_part1_sum_square_total = 0\n",
    "    test_loss_part2_sum_square_total = 0\n",
    "    test_loss_sofa_sum_square_total = 0\n",
    "    for start in pbar:\n",
    "        #print(start)\n",
    "        matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "        input_matrix = matrix[:, :24]\n",
    "\n",
    "        #torch.Size([32, 24, 129])\n",
    "        input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        input_mask = input_matrix[:, :24, V:]\n",
    "        output_matrix = matrix[:, 24:, :V]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        output_mask = matrix[:, 24:, V:]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "        with torch.no_grad():\n",
    "            output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "        for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "            #print(outpu)\n",
    "            a = outpu.item()\n",
    "            second_day = sum(get_sofa(real, var_to_ind))\n",
    "            #print(second_day)\n",
    "            first_day = sum(get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "            c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(second_day, dtype=torch.float32)\n",
    "            sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "\n",
    "    sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "    sofa_var = math.sqrt(sum(x**2 for x in sofa_losses)/len(sofa_losses) - sofa_loss**2)/math.sqrt(len(sofa_losses))\n",
    "    model_name = model_path + \"&\"\n",
    "    sofa_var = torch.tensor(sofa_var)\n",
    "    SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*sofa_var.item())+','+\"{:.3f}\".format(sofa_loss+1.96*sofa_var.item())+ ']}$&'\n",
    "    print(model_name, SOFA_string, sep='',file=open('regression_eicu_partial2.txt','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12355c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_2_PRETRAINED_MODEL_NONE0.05_other_seed.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_2_PRETRAINED_MODEL_NONE0.25.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_4_PRETRAINED_MODEL_NONE0.05.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_1_PRETRAINED_MODEL_NONE0.5.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_3_PRETRAINED_MODEL_NONE0.05.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_1_PRETRAINED_MODEL_NONE0.25.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_8_PRETRAINED_MODEL_NONE0.05.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_1_PRETRAINED_MODEL_NONE0.05.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_3_PRETRAINED_MODEL_NONE0.5.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_7_PRETRAINED_MODEL_NONE0.05.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_6_PRETRAINED_MODEL_NONE0.05_other_seed.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_1_PRETRAINED_MODEL_NONE0.05_other_seed.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_3_PRETRAINED_MODEL_NONE0.05_other_seed.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_4_PRETRAINED_MODEL_NONE0.05_other_seed.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_3_PRETRAINED_MODEL_NONE0.25.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_5_PRETRAINED_MODEL_NONE0.05.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_2_PRETRAINED_MODEL_NONE0.05.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_2_PRETRAINED_MODEL_NONE0.5.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_9_PRETRAINED_MODEL_NONE0.05.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_6_PRETRAINED_MODEL_NONE0.05.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geca-cut_sadlo-k160_s42_w1_f4-SILVER_ONLY_True_n_246080000_5_PRETRAINED_MODEL_NONE0.05_other_seed.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 94/94 [00:21<00:00,  4.29it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import models.InformerAutoregressiveFullRegression as autoformer\n",
    "importlib.reload(autoformer)\n",
    "batch_size, lr, patience = 32, 0.0005, 6  # batch_size increased, patience 10 --> 6\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model = autoformer.Model(args).cuda()\n",
    "# Load pretrained weights here.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=3)\n",
    "for model_path in os.listdir(\"eicu_regression_results_partial_data/\"):\n",
    "    #if not (model_path.endswith(\"1.model\") or model_path.endswith(\"2.model\")): continue\n",
    "    #if not (\"k80\" in model_path or \"k160\" in model_path) or not(model_path.endswith(\"1.model\")): continue\n",
    "    if not (model_path.endswith(\".model\")): continue\n",
    "    print(model_path)\n",
    "    model.load_state_dict(torch.load(\"eicu_regression_results_partial_data/\"+model_path))\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    #print(test_input)\n",
    "    pbar = tqdm(range(0, len(test_input), batch_size))\n",
    "    loss_list = []\n",
    "    test_loss = 0\n",
    "    test_loss_part1 = 0\n",
    "    test_loss_part2 = 0\n",
    "    sofa_losses = []\n",
    "    test_loss_sofa_variables = 0\n",
    "    accuracy_list = []\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    test_loss_sum_square_total = 0\n",
    "    test_loss_part1_sum_square_total = 0\n",
    "    test_loss_part2_sum_square_total = 0\n",
    "    test_loss_sofa_sum_square_total = 0\n",
    "    for start in pbar:\n",
    "        #print(start)\n",
    "        matrix = torch.tensor(test_input[start:start+batch_size], dtype=torch.float32).cuda()\n",
    "        input_matrix = matrix[:, :24]\n",
    "\n",
    "        #torch.Size([32, 24, 129])\n",
    "        input_mark = torch.arange(0, input_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        input_mask = input_matrix[:, :24, V:]\n",
    "        output_matrix = matrix[:, 24:, :V]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        #torch.Size([32, 24])\n",
    "        output_mask = matrix[:, 24:, V:]\n",
    "        #torch.Size([32, 24, 129])\n",
    "        output_mark = torch.arange(0, output_matrix.size(1)).unsqueeze(0).repeat(input_matrix.size(0), 1).cuda()\n",
    "        dec_inp = torch.zeros_like(output_matrix[:, -args.pred_len:, :]).float()\n",
    "        with torch.no_grad():\n",
    "            output = model(input_matrix, input_mark, dec_inp, output_mark, trainn=False)\n",
    "        for outpu, real, mask, input_mat in zip(output, output_matrix, output_mask, input_matrix):\n",
    "            #print(outpu)\n",
    "            a = outpu.item()\n",
    "            second_day = sum(get_sofa(real, var_to_ind))\n",
    "            #print(second_day)\n",
    "            first_day = sum(get_sofa(input_mat, var_to_ind)) #SOFA Source (First Day)\n",
    "            c, d = torch.tensor(a, dtype=torch.float32), torch.tensor(second_day, dtype=torch.float32)\n",
    "            sofa_losses.append(((c-d)**2).sum(axis=-1).mean().item())\n",
    "\n",
    "    sofa_loss = sum(sofa_losses)/len(sofa_losses)\n",
    "    sofa_var = math.sqrt(sum(x**2 for x in sofa_losses)/len(sofa_losses) - sofa_loss**2)/math.sqrt(len(sofa_losses))\n",
    "    model_name = model_path + \"&\"\n",
    "    sofa_var = torch.tensor(sofa_var)\n",
    "    SOFA_string = \"{:.3f}\".format(sofa_loss)+ '$_{['+\"{:.3f}\".format(sofa_loss-1.96*sofa_var.item())+','+\"{:.3f}\".format(sofa_loss+1.96*sofa_var.item())+ ']}$&'\n",
    "\n",
    "    print(model_name, SOFA_string, sep='',file=open('regression_eicu_partial2.txt','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e078d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
